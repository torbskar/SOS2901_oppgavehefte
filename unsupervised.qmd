# Unsupervised learning 
```{r}
#| echo: false
invisible(Sys.setlocale(locale='no_NB.utf8'))
```


I dette kapittelt skal vi bruke følgende pakker: 

```{r}
#| eval: true
#| code-fold: false
#| echo: true
#| warning: false 
#| message: false
library(tidyverse)   # datahåndtering, grafikk og glimpse()
library(skimr)       # funksjonen skim() for å se på data
library(rsample)     # for å dele data i training og testing

library(dendextend)
library(directlabels)

```


"Usupervised learning" er betegnelsen på en type teknikker der vi ikke har et spesifikt observerbart utfall. (Alle andre teknikker for prediksjon kaller vi derfor "supervised learning"). De teknikkene for unsupervised learning vi skal se på her er vanlige former for klustering og datareduksjon. 

Det er to vanlige formål med disse teknikkene: 

1) beskrive et stort og komplisert materiale ved å forenkle til noen færre kategorier 
2) lete etter underliggende strukturer i data som ikke er direkte observerbart 
3) pre-prosessering av data før man fortsetter med supervised learning

Formål 2) er litt kryptisk, men f.eks. en bedrift kan ha nytte av å dele inn kundemassen i ulike segmenter og rette seg inn mot disse på en systematisk måte.^[OBS! Det spiller ingen rolle om disse gruppene er reelle som gruppe eller ikke. At de oppfører seg rimelig likt holder til formålet.] 

For formål 3) gjelder hvis man har veldig mange variable som er korrelerte, men hver for seg ikke er særlig sterke prediktorer. Vanlige prediksjonsmodeller vil ikke alltid være så effektive i slike situasjoner. Forenklede variable som klustre eller principal components kan da fungere bedre. 
En liten advarsel: man ser litt for ofte at denne typen resultater tolkes litt spekulativt i den forstand at klustre eller principal components tillegges en substansiell betydning. Det hender folk sier at de har "identifisert" underliggende klustre eller dimensjoner etc.^[Av høflighetsgrunner er referanser til eksempler utelatt. Men det er ikke vanskelig å finne. Eksemplene er heller ikke avgrenset til akkurat disse metodene, men gjelder også en rekke andre teknikker med tilsvarende formål.] Men altså: klustermetoder lager klustre. Så teknikken lager klustre uansett, og det er ingen magisk innsikt her utover det. Men det kan være nyttig det, altså! 



## K-means klustering 

Vi starter med et tenkt eksempel der det er to variable og tre klustre, men tilhørighet til kluster er ikke direkte observerbart (altså: det er ingen variable for kluster). 

Her ser du de første seks observasjonene i datasettet:

```{r}
#| echo: false
#| warning: false
library(tidyverse)
n <- 100


set.seed(70)
x <- rnorm(n, sd=.5) + rbinom(n, 1, .4)*4
y <- x + rnorm(n)+ ifelse(x<=2, rbinom(n, 1, .5)*6, 0)

df <- data.frame(x = x, y = y)

saveRDS(df, file = "data/kmeans_data.rds")


head(df)

```
Vi kan så plotte x og y i et scatterplot. I dette tilfellet ser vi tydelig at det er tre klynger av datapunkter. Ofte vil det ikke være så lett å se, men disse dataene er laget slik at det skal være lettere å se hvordan algoritmen fungerer. 


```{r}
#| echo: false
#| warning: false
grps <- 3
set.seed(58)
start <- df %>% 
  mutate(ran = runif(dim(.)[1])) %>% 
  arrange(ran) %>% 
  slice(1:grps) %>% 
  select(x, y)

ggplot(df, aes(x=x, y = y)) +
  geom_point(size = 2) +
  scale_shape_manual(values=c(17, 18, 15))+
  geom_point(data = start, col = "purple", shape = 19, size = 2)+
  theme_minimal()+
  theme(legend.position = "none")
```

I plottet er punktene nå lagt inn tre *tilfeldige* punkter som marker startpunktet for algoritmen. Disse punktene er *tilfeldig* valgt, og markerer et første steg som forsøksvis sentrum av tre klustre. Antall klustre må vi altså bestemme selv i forkant. 

For hvert av øvrige datapunktene regnes det så avstanden til hvert av disse tre tilfeldige "klustrene". Hvert datapunkt klassifiseres så til det klusteret de er nærmest. 

```{r}
#| echo: false
#| warning: false
d <- data.frame(grp1 = sqrt( (x-start[1,]$x)^2 + (y-start[1,]$y)^2 ),
                grp2 = sqrt( (x-start[2,]$x)^2 + (y-start[2,]$y)^2 ),
                grp3 = sqrt( (x-start[3,]$x)^2 + (y-start[3,]$y)^2 )) 

df1 <- cbind(df,  d) %>%
  mutate(id = row_number()) %>%
  group_by(id) %>%
  rowwise() %>% 
  mutate(min = min(c_across(grp1:grp3))) %>%
  ungroup() %>% 
  mutate(group = case_when(grp1 == min ~ 1,
                           grp2 == min ~ 2,
                           grp3 == min ~ 3)) %>% 
  group_by(group) %>% 
  select(x, y, id, group) %>% 
  mutate(y_c = mean(y), x_c = mean(x))  



ggplot(df1, aes(x=x, y = y, col = factor(group), shape = factor(group))) +
  geom_point(size = 2) +
  scale_shape_manual(values=c(17, 18, 15))+
  geom_point(aes(x=x_c, y = y_c), col = "purple", shape = 19, size = 2)+
  theme_minimal()+
  theme(legend.position = "none")
```

Etter at hvert datapunkt er klassifisert til det "klusteret" de er nærmest, kan det så regnes det ut midtpunktet for hvert kluster. Dette midtpunktet er så utgangspunktet for neste runde med klassifisering: avstanden fra alle datapunkter til midtpunktet regnes ut og ny klassifisering til det klusteret datapunktet er nærmest.


```{r}
#| echo: false
#| warning: false
start <- df1 %>%
  group_by(group) %>% 
  summarise(x = mean(x), y = mean(y)) %>% 
  select(x, y) 

d <- data.frame(grp1 = sqrt( (x-start[1,]$x)^2 + (y-start[1,]$y)^2 ),
                grp2 = sqrt( (x-start[2,]$x)^2 + (y-start[2,]$y)^2 ),
                grp3 = sqrt( (x-start[3,]$x)^2 + (y-start[3,]$y)^2 )) 


df2 <- cbind(df1[,1:2],  d) %>% 
  mutate(group = df1$group) %>% 
  mutate(id = row_number()) %>%
  group_by(id) %>%  
  rowwise() %>% 
  mutate(min = min(c(grp1, grp2, grp3))) %>%
  ungroup() %>% 
  mutate(group = case_when(grp1 == min ~ 1,
                           grp2 == min ~ 2,
                           grp3 == min ~ 3)) %>% 
  group_by(group) %>% 
  mutate(y_c = mean(y), x_c = mean(x))

ggplot(df2, aes(x=x, y = y, col = factor(group))) +
  geom_point(size = 2) +
  scale_shape_manual(values=c(17, 18, 15))+
  geom_point(aes(x=x_c, y = y_c), col = "purple", shape = 19, size = 2)+
  theme_minimal()+
  theme(legend.position = "none")
```

Etter ny klassifisering, regnes det så ut et nytt midtpunkt og vi gjør det hele en gang til: regner ut avstanden og klassifiserer til nærmeste.  


```{r}
#| echo: false
#| warning: false
start <- df2 %>%
  group_by(group) %>% 
  summarise(x = mean(x), y = mean(y)) %>% 
  select(x, y) 


d <- data.frame(grp1 = sqrt( (x-start[1,]$x)^2 + (y-start[1,]$y)^2 ),
                grp2 = sqrt( (x-start[2,]$x)^2 + (y-start[2,]$y)^2 ),
                grp3 = sqrt( (x-start[3,]$x)^2 + (y-start[3,]$y)^2 )) 
 
df3 <- cbind(df2[,1:2],  d) %>% 
  mutate(group = df2$group) %>% 
  mutate(id = row_number()) %>%
  group_by(id) %>%  
  rowwise() %>% 
  mutate(min = min(c(grp1, grp2, grp3))) %>%
  ungroup() %>% 
  mutate(group = case_when(grp1 == min ~ 1,
                           grp2 == min ~ 2,
                           grp3 == min ~ 3)) %>% 
  group_by(group) %>% 
  mutate(y_c = mean(y), x_c = mean(x))

ggplot(df3, aes(x=x, y = y, col = factor(group))) +
  geom_point(size = 2) +
  scale_shape_manual(values=c(17, 18, 15))+
  geom_point(aes(x=x_c, y = y_c), col = "purple", shape = 19, size = 2)+
  theme_minimal()+
  theme(legend.position = "none")
```

Et nytt midtpunkt regnes ut og vi gjør det hele enda en gang.

```{r}
#| echo: false
#| warning: false
start <- df3 %>%
  group_by(group) %>% 
  summarise(x = mean(x), y = mean(y)) %>% 
  select(x, y) 

d <- data.frame(grp1 = sqrt( (x-start[1,]$x)^2 + (y-start[1,]$y)^2 ),
                grp2 = sqrt( (x-start[2,]$x)^2 + (y-start[2,]$y)^2 ),
                grp3 = sqrt( (x-start[3,]$x)^2 + (y-start[3,]$y)^2 )) 
df4 <- cbind(df3[,1:2],  d) %>% 
  mutate(group = df3$group) %>% 
  mutate(id = row_number()) %>%
  group_by(id) %>%  
  rowwise() %>% 
  mutate(min = min(c(grp1, grp2, grp3))) %>%
  ungroup() %>% 
  mutate(group = case_when(grp1 == min ~ 1,
                           grp2 == min ~ 2,
                           grp3 == min ~ 3)) %>% 
  group_by(group) %>% 
  mutate(y_c = mean(y), x_c = mean(x))


ggplot(df4, aes(x=x, y = y, col = factor(group))) +
  geom_point(size = 2) +
  scale_shape_manual(values=c(17, 18, 15))+
  geom_point(aes(x=x_c, y = y_c), col = "purple", shape = 19, size = 2)+
  theme_minimal()+
  theme(legend.position = "none")

```

Merk nå at i siste runde var det ingen av punktene som byttet kluster. Da avsluttes algoritmen og alle punkter er klassifisert etter hvilket kluster de er mest lik de andre punktene. 

I dette eksempelet er klusterne tydelig separert og det er forholdsvis lett å gjøre klassifiseringen. I andre tilfeller er det ikke nødvendigvis like greit, og det må langt flere iterasjoner til før det landes på en løsning. 

Det er verd å merke seg at startpunktet (altså de tre tilfeldige punktene) kan ha betydning for løsningen. Det er derfor vanlig at software tester ut flere startverdier og velger den løsningen som passer best. 





### K-means med `kmeans()` 

Vi bruker eksempeldatasettet brukt ovenfor. 

```{r}
#| echo : false
#| warning: false
#| eval: false
df <- readRDS("data/kmeans_data.rds")
head(df)
```



Det er en del plunder med å gjøre kmeans manuelt. Heldigvis er det en funksjon som fikser hele saken for oss. Funksjonen `kmeans()` trenger primært tre imput: de dataene som skal klustres, antall klustere, og antall tilfeldige startverdier. Antall klustre må vi altså bestemme selv. 

Her er en kode som først kjører kmeans-algoritmen. I det objektet som kommer ut er det en vektor som heter `...$cluster` som er selve klassifiseringen. Denne kan vi legge inn som en ny variabel i datasettet. Merk at rekkefølgen på observasjonene og output-objektet er den samme, så det blir riktig å bare legge til vektoren som en ny variabel. Dernest kan vi plotte resultatet. 

```{r}
km_df <- kmeans(df, centers = 3, nstart=20)

df_p <- df %>% 
  mutate(cluster = km_df$cluster)


ggplot(df_p, aes(x=x, y = y, col = factor(cluster), shape = factor(cluster))) +
  geom_point()


```

Dette ble selvfølgelig likt som i den trinnvise prosedyren vist ovenfor. Det kan være greit å være klar over at navnet på hvert kluster (1, 2 eller 3) er tilfeldig og du kan få en annen rekkefølge på navnene en annen gang. Det spiller ingen rolle, men er lett å bli forvirret av. 


### Hvor mange klustre trenger man? 
Tja. Hvis du har en god tanke om hvor mange klustre du trenger, så er jo saken grei. Da bruker du disse. Hvis du derimot ikke vet - og det er det vanlige - så kan du gå for det antallet som best oppsummerer dataene. Men hva er så det? Et mål er summen av avstandene til sentrum innenfor hver kluster. Omtales gjerne som "within total sum of squares". Den løsningen (dvs. antall klustre) som gir lavest kvadratsummer er da den som er "best". 

I praksis betyr det at man tilpasser modellen flere ganger, med trinnvis flere klustre. Så kan man sammenligne "within total sum of squares". 

Den etterfølgende koden gjør dette. Den er litt krøkete, dessverre da den innebærer å skrive en loop.^[Det er muligens greit for de av dere som har lært en god del R tidligere. Ikke like greit for dere andre. ] Her er i hvert fall full kode. 

```{r}
wss <- 0
# For 1 to 15 cluster centers
for (i in 1:5) {
  km.out <- kmeans(df, centers = i, nstart=20)
  # Save total within sum of squares to wss variable
  wss[i] <- km.out$tot.withinss
}

# Plot total within sum of squares vs. number of clusters
plot(1:5, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")
# Marker "albuen" med en linje i plottet 
abline(v=3, col="red")

```

Kvadratsummen reduseres for antall klustre. Intuitivt er jo det rimelig: jo flere klustre - jo kortere er avstanden til et kluster-sentrum. Men merk at trenden flater ganske tydelig ut etter 3 klustre. Altså: forbedringen i tilpassning er minimal. Et slikt plot kalles "elbow method" eller "scree plot". Beste modell er der hvor kurven får en "albue", eller med andre ord: der reduksjonen avtar. Det er ikke alltid det er lett å bedømme, men her er det ikke en egentlig fasit. Det finnes ingen fasit hvis man ikke fra før av vet at det finnes et gitt antall klustre. 


### Longitudinelle data
Det er nok vanligst å klustre ulike variable. Men hvis variablene innholder verdier for ulike tidsenheter (f.eks. per år), så kan vi også analysere tidstrender på denne måten. Antall variable som klustres kan være flerdimensjonalt. Ovenfor er det brukt bare to variable, men det normale er jo at det er langt flere dimensjoner samtidig. 






## Datareduksjon med principal component analysis (PCA)

En annen form for unsupervised learning er principal component analysis (PCA). Dette er en teknikk som reduserer dimensjonaliteten i et datasett. Med dimensjonalitet mener vi i praksis antall variable. I et datasett med mange variable kan det være vanskelig å se sammenhenger og strukturer. PCA reduserer antall variable til et fåtall "hovedkomponenter" som forklarer mesteparten av variansen i datasettet. Det man får ut av PCA er nye variable som er lineære kombinasjoner av de opprinnelige variablene. Den første hovedkompenenten er den som forklarer mest av varians i variablene. Den andre forklarer nest mest osv. For den første principal component (PC1) får hver variabel en *factor loading* som forteller hvor mye variabelen bidrar til PC1. 

Det som noen ganger kalles *principal component regression* er en variant der man bruker den første hovedkomponenten som prediktor i en regresjonsanalyse. Dette tilsvarer å lage en indeks av de opprinnelige variablene. 


### Empirisk eksempel
Vi starter med datasettet for kriminalitet i norske kommuner, og velger en enkelt årgang for enkelhets skyld. Vi beholder et begrenset sett av variable av ulik art.  

```{r}
#| warning: false
#| message: false
kommune <- readRDS( "data/kommunedata.rds") %>% 
  filter(year == 2020) %>% 
    mutate(kommune = ifelse(kommune == "Oslo municipality", "Oslo", kommune)) %>%
  select(kommune, menn_18_25:menn_18min, inntekt_totalt_median:andre_lovbrudd) %>% 
  select(-inntekt_eskatt_median) %>% 
  column_to_rownames(var = "kommune")

glimpse(kommune)
```

Så et lite triks som ikke er viktig for resultatet som sådan, men hjelper i visualiseringen litt senere er å bruke radnavn. Vi er kjent med at kolonnene i et datasett har navn: altså variabelnavn. Men i R kan også *radene* ha navn. Det er ikke like vanlig å bruke til noe spesielt og kan med fordel unngås til vanlig. Men akkurat her gjør det at kommunenavnene vises når vi bruker funksjonen `biplot` nedenfor. Analyse vil ellers fungere like godt uten denne koden. 

Kodesnutten `column_to_rownames(var = "kommune")` gjør om variabelen *kommune* til radnavn. Da er det bare de numeriske verdiene igjen som skal analyseres.^(Hvis ikke måtte første kollonne fjernes manuelt først før PCA.)

Hvis vi skal visualisere dataene kan man bruke et scatterplot matrise, der alle par av variable plottes mot hverandre. Dette kan gjøres med `GGally`-pakken, og her er det valgt ut bare noen få av variablene. 

```{r}
#| warning: false
#| message: false
library(GGally)
ggpairs(kommune[ ,c(1, 3, 6, 7, 8, 10, 11)])

```


Dette var bare noen få variable, og det gir rett og slett ikke så mye sammenheng. Med flere variable blir det enda vanskeligere å få noe ut av dette. 



I stedet kan vi kjøre en principal component analyse med `prcomp`. Det er her viktig å sette `scale = TRUE` og `center = TRUE`. Dette er innebygde funksjoner som standardiserer dataene før algoritmen kjøres. Altså: dataene re-skaleres til en z-skår med gjennomsnitt 0 og standardavvik 1. Det skal veldig gode grunner for å gjøre noe annet, så gjør alltid dette. Koden nedenfor viser hvordan dette gjøres og gir en oppsummering av resultatet. 


```{r}
library(stats)
pr_komm <- prcomp(kommune, scale = TRUE, center = TRUE)

summary(pr_komm)

```

Resultatet vises som en liste over de ulike komponentene. Den første komponenten forklarer mest av variansen, den andre nest mest osv. I dette tilfellet er det 15 komponenter. I praksis bryr vi oss mest om de første par komponentene hvis de forklarer tilstrekkelig av variansen. Her fanger de 3 første komponente opp over 80% av variansen. 

PCA-objektet inneholder flere elementer som vi skal bruke nedenfor. En enkelt måte å få innsikt på er å bruke `names`-funksjonen slik: `names(pr_komm)`. Men det er først og fremst `pr_komm$x` og `pr_komm$sdev` som er interessante. `x` en matrise med de nye variablene som er lineære kombinasjoner av de opprinnelige variablene, altså et datasett men i matriseformat. `sdev` er standardavviket for de ulike komponentene, som vistes i første linjen i `summary(pr_komm)`.


Det er vanlig å undersøke dette visuelt med et såkalt biplot. Dette er et scatterplott av PC1 og PC2 der hver enhet plasseres i forhold til disse. I tillegg vises factor loadings for hver variabel som piler. Dette plottet kan lages manuelt med `ggplot`, men det er enklere å bruke `biplot`-funksjonen som er spesielt til dette formålet. Argumentene `cex =  `,  `cex.axis =  ` og `cex.lab = ` er bare for å justere størrelsen på teksten. 

```{r}
par(mar=c(2,1,2,1))
biplot(pr_komm, cex = .4, cex.axis = .6, cex.lab = .6)

```
Det skal innrømmes at dette plottet er heller ikke så lett å tolke uten videre, men først og fremst fordi en del tekst kommer oppå hverandre. Så kanskje bør man ta en titt på tabellen for factor loadings for de to første komponentene i tillegg.

Det er en del informasjon i dette plottet. Pilene viser *factor loadings* for hver variabel som retningen og lengden på pilen. Jo lengre pilen er, jo mer bidrar variabelen til den aktuelle komponenten. Navnet til hver enkelt kommune plassert i forhold til de ulike komponentene.^(Ovenfor var en kodesnutt som la kommunenvanene som radnavn. Hvis du ikke gjør dette kommer det punkter i stedet for navn.) Hvis to kommuner er plassert nærme hverandre, så er de like i forhold til de ulike komponentene. Hvis de er langt fra hverandre, så er de mer ulike. Vi ser bl.a. at Oslo skiller seg markant fra alle andre, men at Bergen og Trondheim er ganske like i samme retning. Eidskog og Storfjord ligger i en ganske annen retning. 

Factor loadings viser hvor mye hver variabel bidrar til den aktuelle komponenten. Dette kan hentes ut med `pr_komm$rotation`.

```{r}
pr_komm$rotation[,1:2]

```

Variablene knyttet til befolkningsstruktur, husholdninger og sosialhjelp er de som bidrar mest til PC1. Variablene knyttet til kriminalitet er de som bidrar mest til PC2. Dette er en ganske vanlig situasjon: at ulike typer variabler grupperer seg sammen. 

Vinningskriminalitet og voldskriminalitet har ganske lik factor loading. Det samme gjelder for ordenslovbrudd og lovbrudd knyttet til narkotika eller alkohol. Trafikklovbrudd skiller seg derimot en del fra de andre. 


Det er også mulig å hente ut annen informasjon fra PCA-objektet. 
For å se på factor loadings for hver kommune kan vi undersøke `x`-matrisen i pca-objektet slik:  

```{r}
head(pr_komm$x)

```
Dette er en matrise med de nye variablene som er lineære kombinasjoner av de opprinnelige variablene. Hver rad er en kommune, og hver kolonne er en utregnet komponent. 

Standardavviket for de ulike komponentene er gitt i `pr_komm$sdev`. Dette kan brukes til å regne ut andelen av variansen som forklares av hver komponent. Vi kan også plotte dette. 

```{r}
pvar <- pr_komm$sdev^2   # kvadrerte standardavvik (dvs. variansen)
pve <- pvar/sum(pvar)    # Hver komponents andel av variansen


# Hver komponents andel av variansen lagres i en data.frame for å kunne plottes med ggplot
dt <- data.frame(components = 1:length(pve), prop_var_expl = pve)

ggplot(dt, aes(x = components, y = prop_var_expl))+ 
  geom_line()+
  geom_point()
 

```

Dette er en visualisering av hvor mye hver komponent bidrar til å forklare variansen i datasettet. Vi ser at de første komponentene forklarer mest, og at det flater ut etter hvert. Der hvor kurven flates ut med en "albue" er et godt sted å stoppe. I dette tilfellet er det etter 3 komponenter som summerer opp det meste av dataene. 

Slik sett er datasettet redusert fra 15 variable til 3. Der de opprinnelige variablene var korrelerte i varierende grad, så er de tre nye variablene ikke korrelerte, men representerer ulike dimensjoner. Merk at de nye variablene beholder det aller meste av informasjonen i de opprinnelige dataene (jf. plottet over), og er mye mer effektivt enn å bruke alle de opprinnelige variablene i en regresjonsanalyse eller bare plukke ut noen få som tilsynelatende er viktigst isolert sett. 

