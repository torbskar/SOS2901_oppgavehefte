
## Noen innledende metodiske begrep
I standard samfunnsvitenskapelige metodekurs lærer man først og fremst teknikker for å beskrive data og statistisk inferens for å beskrive usikkerheten rundt estimatene. Avhengig av studiets øvrige design kan resultatene tolkes kausalt og/eller generalisere til en nærmere veldefinert populasjon [@berk2016a]. 

Usikkerhet beskrives typisk ved hjelp standardfeil, p-verdier og konfidensintervall tilhørende spesifikke statistiske tester. Dette innebærer at man bruker statistiske *modeller* for hvordan resultatene ville sett ut under spesifikke forutsetninger. Samplingfordelinger som normalfordelingen og en del andre tilsvarende fordelinger er derfor sentralt. De fleste teknikkene vi skal bruke i dette kurset er *ikke* statistiske modeller i samme forstand og det er ingen antakelser om samplingfordelinger. Standardfeil og konfidensintervall kan derfor ikke regnes ut. Usikkerhet og hvem resultatene gjelder for er også relevante for maskinlæring, men ikke helt på samme måte. 



## Forklaringer og prediksjoner 

Vi er i liten grad interessert i regresjonskoeffisienter, $\beta$, og tolkning av denne. Derimot er vi interessert i det predikerte utfallet $\hat{y}_i$. 


## Overfitting: training og testing dataset 

Når man tilpasser en statistisk modell eller algoritme til data så er det lett å tenke at modellen bør gjenspeile dataene så godt som mulig. Samtidig sies det ofte at modellene skal være så enkle som tilrådelig. En mer komplisert modell vil jo være i stand til å tilpasses dataene i større grad, så hvordan avveie dette? 

Spørsmålet nå er ikke hvor godt modellen passer til disse dataene, men hvordan den passer til *nye* data! Altså *fremtidige* data eller fremtidig situasjon. La oss si at vi har et datasett som kan plottes om følger:

```{r}
library(tidyverse)
n <- 10
beta <- 1
set.seed(42)
x <- round(20 + runif(n)*50, digits = 1)
y <- 1 + beta*x + rnorm(n)*10

df <- data.frame(x = x, y = y) %>% 
  mutate(d = case_when(x < 50 ~ 0,
                       x < 56 ~ 1, 
                       TRUE ~2) %>% as_factor())


g1 <- ggplot(df, aes(x = x, y = y)) +
  geom_point() 
g1


```

Vi kunne her tilpasse en enkel lineær regresjonsmodell eller en mer komplisert modell. Resultatet vises i grafen nedenfor. 


```{r}

est1 <- lm(y ~ x + x*d, data = df)
est2 <- lm(y ~ x, data = df)

df_p1 <- df %>% 
  mutate(pred = predict(est1))  %>% 
  mutate(res_pred = y - pred, 
         res_y = y - mean(y))

df_p1 %>% 
  summarise(res_pred = sum(res_pred^2), 
            res_y = sum(res_y^2)) %>% 
  mutate(1 - res_pred/res_y)

ggplot(df, aes(x = x, y = y)) +
  geom_point(col = "black") +
  geom_line(data = df_p1, aes(y = pred), col = "red", linewidth = .7) +
  stat_smooth(method='lm', formula = y ~ x, se = F, col = "blue", linewidth = .7)
```

Den kompliserte modellen gir $r^2$ = `r summary(est1)$r.squared` mens den enkle lineære gir $r^2$ = `r summary(est2)$r.squared`. 




```{r}
summary(est1)$r.squared
summary(est2)$r.squared


```




```{r}
x <- round(20 + runif(n)*50, digits = 1)
y <- 1 + beta*x + rnorm(n)*10
df2 <- data.frame(x = x, y = y)
  
g1 +
  geom_point(data = df2, col = "blue") +
  geom_smooth(method = lm, se = F, col = "red")
```





## Klassifikasjonsusikkerhet - grunnleggende begreper 

### falske positive og negative 
Når vi predikerer et kategorisk utfall er det gjerne ett av utfallene vi primært er interessert i. Disse kalles *positive* og de andre er *negative*. Dette har ingenting å gjøre med om utfallet er bra eller dårlig å gjøre. Å predikere en sykdom vil være *positivt* og å være frisk vil være *negativt*. Å ha tilbakefall til kriminalitet vil være *positivt* og lovlydig vil være *negativt*. 

En *positiv* prediksjon kan da være korrekt eller feil, og disse kalles da henholdsvis *sanne* eller *falske* positive. Tilsvarende kan en negaitv prediksjon være sann eller falsk. 

### Confusion matrix 




### Asymetriske kostnader




## Rettferdighet og rimelighet
I diskusjoner av anvendelser av maskinlæring står rettferdighet helt sentralt. Men det er ikke alltid like klart hva dette egentlig betyr utover at det er forskjellsbehandling. Tross alt er hele formålet med prediksjon å nettopp forskjellsbehandle, eller *målrette* som det også kan kalles. 


### Fundamentale skjvheter i data
Siden maskinlæring baserer seg på å lære av tilgjengelige data for å benytte det på nye tilfeller spiller det vesentlig rolle hvordan de opprinnelige dataene ble generert i utgangspunktet. 

Et velkjent eksempel er hvordan [Amazon besluttet å slutte å bruke en algoritme for rekruttering fordi den systematisk valgte bort kvinner](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G). Grunnen til at algoritmen gjorde dette var så enkelt som at dataene den var trent opp på var mannsdominert. Algoritmen hadde altså primært tilgang til informasjon om hvilke egenskaper som kjennetegnet talentfulle *mannlige* kandidater, som altså kan være forskjellige fra talentfulle *kvinnelige* kandidater. 

Når man skal ta en algoritme i bruk er det derfor helt avgjørende at man kan forsvare bruken av de dataene algoritmen er trent på. Kjente skjevheter kan i prinsippet motarbeides ved *tuning* (dette kommer vi tilbake til), men det er vanskelig å garantere at det er skjevheter man *ikke* har tenkt på. 


### Urimelige feilrater 



### Ulike feilrater på tvers av undergrupper



